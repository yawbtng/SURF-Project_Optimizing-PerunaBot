{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Set Up\n",
    "This notebook is for the initial set up of the LLM application that will be the same for each model.\n",
    "\n",
    "The focus is on the data-pre processing step of the RAG pipeline and getting the data into the vector database.\n",
    "\n",
    "We need to put the data of the SMU Catalog of 2023-2024 into Qdrant which is a cloud vector database. This will allow the language model to access and retrieve the necessary information.\n",
    "\n",
    "There are many changes that can be done at this step to alter how the text goes into the vector database (ex: different text splitters, document loaders, etc.)\n",
    "\n",
    "We will install some of the necessary dependancies now and the rest along the way throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pypdf qdrant-client langchain python-dotenv tiktoken langchain-openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up to initialize API keys from .env file into the\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Load environment variables from the .env files\n",
    "load_dotenv(find_dotenv(filename='SURF-Project_Optimizing-PerunaBot/setup/.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you take a look at the pdfs in the data folder, the catalog pdf is over 800 pages long! To upload it into the vector databse, we have to 1st get all the text from the documents.\n",
    "\n",
    "First we will do the PDFs, then later on the CSV of the FAQs with a different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Southern Methodist University \n",
      "General Information \n",
      "Undergraduate Catalog  \n",
      "2023 -2024  \n",
      "{'source': 'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/20232024 Undergraduate Catalog91123.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# langchain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "text = \"hi\"\n",
    "# file paths to the two PDFs we're using\n",
    "pdf_paths = ['C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/20232024 Undergraduate Catalog91123.pdf',\n",
    "             'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/Official University Calendar 2023-2024.pdf'\n",
    "             ]\n",
    "\n",
    "def load_pdfs_with_langchain(pdf_paths):\n",
    "    documents = []\n",
    "    for path in pdf_paths:\n",
    "        try:\n",
    "            # Use LangChain's PyPDFLoader to load the PDF\n",
    "            loader = PyPDFLoader(path)\n",
    "            # Load and pase the PDF into document instances\n",
    "            pdf_doc = loader.load()\n",
    "            # Insert pdf into documents list variable\n",
    "            documents.extend(pdf_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "#Load PDF documents using the function\n",
    "docs = load_pdfs_with_langchain(pdf_paths)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are initializing an API connection to the Qdrant vector database from https://qdrant.tech/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import qdrant_client\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Qdrant host URL and API key\n",
    "qdrant_host = os.environ['QDRANT_HOST']\n",
    "qdrant_api_key = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "#Initialize Qdrant Client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=qdrant_host, \n",
    "    api_key = qdrant_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the 1st collection of vectors and the vectorstore inside the database. Eventually, we will have more than one collection to see how changes to how the data is uploaded and retrieved affects the accuracy and other evaluation metrics.\n",
    "\n",
    "This is a very important step because an LLM application is only as good as its data and the documents it retrieves to create an answer.\n",
    "\n",
    "Since we will have more than one collection within the vector database, we will just create a function that will allow us to create a new vectorstore collection when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vector store based on the collection name\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "# Initializing OpenAI API key for embeddings and later use\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def create_vectorstore(qdrant_collection_name):\n",
    "    \n",
    "    vectors_config = models.VectorParams(\n",
    "   size=1536, #for OpenAI\n",
    "   distance=models.Distance.COSINE\n",
    "   )\n",
    "    \n",
    "    client.create_collection(\n",
    "   collection_name = qdrant_collection_name,\n",
    "   vectors_config=vectors_config,   \n",
    "    )\n",
    "\n",
    "    vector_store = Qdrant(\n",
    "        client=client, \n",
    "        collection_name=qdrant_collection_name, \n",
    "        embeddings=OpenAIEmbeddings(),\n",
    "    )\n",
    "  \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1st collection of vectors\n",
    "\n",
    "qdrant_collection_1 = os.environ['QDRANT_COLLECTION_1']\n",
    "vector_store_1 = create_vectorstore(qdrant_collection_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the experiment begins! Now we have all the text from the pdfs in the pdfs_doc_text variable, the text needs to be split into chunks using langchain text splitters to be turned into vectors using the OpenAI Embeddings Model.\n",
    "\n",
    "We are going to test two methods:\n",
    "1. Parent Doucment Retriever method with the **RecursiveCharacterTextSplitter**\n",
    "\n",
    "2. Semantic Chunking method using the **Semantic Text Splitter**\n",
    "\n",
    "Since this will generate two different types of chunks, we will but them in two different collections within the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Document Retriever Method\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=25, \n",
    "                                                length_function=len, add_start_index=True) \n",
    "\n",
    "parent_splitter =RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=50, \n",
    "                                                length_function=len, add_start_index=True)  \n",
    "\n",
    "# storage for parent splitter\n",
    "store = InMemoryStore()\n",
    "\n",
    "# retriever\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store_1, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    )\n",
    "\n",
    "parent_retriever.add_documents(docs)\n",
    "# testing the retriever\n",
    "parent_retriever.invoke(\"Tell me about Computer Science at SMU\")\n",
    "\n",
    "# adding  documents into the Qdrant vector database in the 1st collection\n",
    "parent_retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the 2nd method using the Semantic Text Splitter which splits the text based on the meaning within each sentence for more granular control of retrieval.\n",
    "\n",
    "For this one, we will use the Ensemble Retriever which allows us to combine the results of multiple retrievers, giving them different weights. Within the Ensemble retriever we will use: \n",
    "-BM25 Retriever which a retrieval method used by search engines\n",
    "-Base retriever that comes with using the vectorstore as a retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Southern Methodist University \n",
      "General Information \n",
      "Undergraduate Catalog  \n",
      "2023 -2024   \n",
      "2135\n"
     ]
    }
   ],
   "source": [
    "# semantic text splitting method\n",
    "# do '%pip install langchain_experimental' if needed\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), \n",
    "    breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "semantic_docs = semantic_text_splitter.split_documents(docs)\n",
    "print(semantic_docs[0].page_content)\n",
    "print(len(semantic_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install rank_bm25\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever\n",
    "# we already imported the Qdrant vector store and OpenAI embeddings in a previous step\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(semantic_docs)\n",
    "\n",
    "# creating another instance of a vector store with a new collection using the function we made earlier\n",
    "qdrant_collection_2 = os.environ['QDRANT_COLLECTION_2']\n",
    "vector_store_2 = create_vectorstore(qdrant_collection_2)\n",
    "\n",
    "vector_store_2.from_documents(semantic_docs, OpenAIEmbeddings())\n",
    "vector_store_2_retriever = vector_store_2.as_retriever()\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_store_2_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# adding the semantically split docs into the vector store\n",
    "vector_store_2_retriever.add_documents(semantic_docs)\n",
    "\n",
    "ensemble_retriever.invoke(\"Tell me about Computer Science at SMU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to work on uploading the excel file of 115 FAQs into each vector database collection. There are multiple sheets within the excel file so we're going to have to turn each sheet into a CSV using the pandas library and then use langchain's CSV loader to turn them into langchain documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pandas library to work with the excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = 'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/SMU FAQs.xlsx'\n",
    "xlsx = pd.ExcelFile(excel_path)\n",
    "\n",
    "# checking to see if loading the file worked\n",
    "print(xlsx.sheet_names)\n",
    "\n",
    "# Iterate through each sheet and save as a CSV file\n",
    "csv_files = []\n",
    "for sheet_name in xlsx.sheet_names:\n",
    "     # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(xlsx, sheet_name=sheet_name)\n",
    "    \n",
    "    # Extract metadata from cell A1\n",
    "    metadata = df.iat[0, 0]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = f'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/{sheet_name}.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    csv_files.append((csv_path, metadata))\n",
    "\n",
    "# Display the list of generated CSV files and their metadata\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the specific CSV file\n",
    "unwanted_csv = 'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/SMU FAQs.csv'\n",
    "filtered_csv_files = [file for file in csv_files if file[0] != unwanted_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now turning each csv into a langchain document\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Create LangChain documents from CSV files with metadata\n",
    "csv_documents = []\n",
    "\n",
    "for csv_path, metadata in csv_files:\n",
    "    loader = CSVLoader(file_path=csv_path)\n",
    "    csv_docs = loader.load()\n",
    "    for csv_doc in csv_docs:\n",
    "        csv_doc.metadata['source'] = metadata\n",
    "    csv_documents.extend(docs)\n",
    "\n",
    "# Display the first document as an example\n",
    "print(csv_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
