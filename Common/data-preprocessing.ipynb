{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Pre-processing\n",
    "This notebook is for the initial set up of the LLM application that will be the same for each model.\n",
    "\n",
    "The focus is on the data-pre processing step of the RAG pipeline and getting the data into the vector database.\n",
    "\n",
    "We need to put the data of the SMU Catalog of 2023-2024 into Qdrant which is a cloud vector database. This will allow the language model to access and retrieve the necessary information.\n",
    "\n",
    "There are many changes that can be done at this step to alter how the text goes into the vector database (ex: different text splitters, document loaders, retrievers, etc.)\n",
    "\n",
    "We will install some of the necessary dependancies now and the rest along the way throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pypdf qdrant-client langchain python-dotenv tiktoken langchain-openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to initialize API keys from .env file into the\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Load environment variables from the .env files\n",
    "load_dotenv(find_dotenv(filename='SURF-Project_Optimizing-PerunaBot/setup/.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will initialize langsmith for tracing and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "langchain_api_key = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]\n",
    "langchain_endpoint = os.environ[\"LANGCHAIN_ENDPOINT\"]\n",
    "langsmith_project = os.environ[\"LANGCHAIN_PROJECT\"]\n",
    "\n",
    "langsmith_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Breakdown\n",
    "If you take a look at the pdfs in the data folder, just the catalog pdf alone is over 800 pages long! To upload it into the vector databse, we have to 1st get all the text from the documents.\n",
    "\n",
    "First we will do the PDFs, then later on the CSV of the FAQs with a different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# file paths to the two PDFs we're using\n",
    "pdf_paths = ['C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/20232024 Undergraduate Catalog91123.pdf',\n",
    "             'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/Official University Calendar 2023-2024.pdf',\n",
    "             'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/2023_PerunaPassport.pdf',\n",
    "             'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/SMU Student Handbook 23-24.pdf',\n",
    "             'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/SMUCampusGuideFactsMap.pdf'\n",
    "             ]\n",
    "\n",
    "def load_pdfs_with_langchain(pdf_paths):\n",
    "    documents = []\n",
    "    for path in pdf_paths:\n",
    "        try:\n",
    "            # Use LangChain's PyPDFLoader to load the PDF\n",
    "            loader = PyPDFLoader(path)\n",
    "            # Load and pase the PDF into document instances\n",
    "            pdf_doc = loader.load()\n",
    "            # Insert pdf into documents list variable\n",
    "            documents.extend(pdf_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "#Load PDF documents using the function\n",
    "docs = load_pdfs_with_langchain(pdf_paths)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are initializing an API connection to the Qdrant vector database from https://qdrant.tech/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import qdrant_client\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Qdrant host URL and API key\n",
    "qdrant_host = os.environ['QDRANT_HOST']\n",
    "qdrant_api_key = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "#Initialize Qdrant Client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=qdrant_host, \n",
    "    api_key = qdrant_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create the 1st collection of vectors and the vectorstore inside the database. Eventually, we will have more than one collection to see how changes to how the data is uploaded and retrieved affects the accuracy and other evaluation metrics.\n",
    "\n",
    "This is a very important step because an LLM application is only as good as its data and the documents it retrieves to create an answer.\n",
    "\n",
    "Since we will have more than one collection within the vector database, we will just create a function that will allow us to create a new vectorstore collection when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a vector store based on the collection name\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "# Initializing OpenAI API key for embeddings and later use\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def create_vectorstore(qdrant_collection_name):\n",
    "    \n",
    "    vectors_config = models.VectorParams(\n",
    "   size=1536, #for OpenAI\n",
    "   distance=models.Distance.COSINE\n",
    "   )\n",
    "    \n",
    "    client.create_collection(\n",
    "   collection_name = qdrant_collection_name,\n",
    "   vectors_config=vectors_config,   \n",
    "    )\n",
    "\n",
    "    vector_store = Qdrant(\n",
    "        client=client, \n",
    "        collection_name=qdrant_collection_name, \n",
    "        embeddings=OpenAIEmbeddings(),\n",
    "    )\n",
    "  \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1st collection of vectors\n",
    "\n",
    "qdrant_collection_1 = os.environ['QDRANT_COLLECTION_1']\n",
    "vector_store_1 = create_vectorstore(qdrant_collection_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the experiment begins! Now we have all the text from the pdfs in the pdfs_doc_text variable, the text needs to be split into chunks using langchain text splitters to be turned into vectors using the OpenAI Embeddings Model.\n",
    "\n",
    "We are going to test two methods:\n",
    "1. Parent Doucment Retriever method with the **RecursiveCharacterTextSplitter**\n",
    "\n",
    "2. Semantic Chunking method using the **Semantic Text Splitter**\n",
    "\n",
    "Since this will generate two different types of chunks, we will but them in two different collections within the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Document Retriever Method\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=25, \n",
    "                                                length_function=len, add_start_index=True) \n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=50, \n",
    "                                                length_function=len, add_start_index=True)  \n",
    "\n",
    "# storage for parent splitter\n",
    "store = InMemoryStore()\n",
    "\n",
    "# retriever\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_store_1, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    )\n",
    "# adding  documents into the Qdrant vector database in the 1st collection\n",
    "parent_retriever.add_documents(docs)\n",
    "\n",
    "# testing the retriever\n",
    "parent_retriever.invoke(\"How many credit hours is a major in Computer Science?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the 2nd method using the Semantic Text Splitter which splits the text based on the meaning within each sentence for more granular control of retrieval.\n",
    "\n",
    "For this one, we will use the Ensemble Retriever which allows us to combine the results of multiple retrievers, giving them different weights. Within the Ensemble retriever we will use: \n",
    "\n",
    "- BM25 Retriever which a retrieval method used by search engines\n",
    "- Base retriever that comes with using the vectorstore as a retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic text splitting method\n",
    "# do '%pip install langchain_experimental' if needed\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), \n",
    "    breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "semantic_docs = semantic_text_splitter.split_documents(docs)\n",
    "print(semantic_docs[0].page_content)\n",
    "print(len(semantic_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating another instance of a vector store with a new collection using the function we made earlier\n",
    "qdrant_collection_2 = os.environ['QDRANT_COLLECTION_2']\n",
    "vector_store_2 = create_vectorstore(qdrant_collection_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever\n",
    "# we already imported the Qdrant vector store and OpenAI embeddings in a previous step\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(semantic_docs)\n",
    "\n",
    "# vector_store_2.from_documents(semantic_docs, OpenAIEmbeddings())\n",
    "vector_store_2_retriever = vector_store_2.as_retriever()\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_store_2_retriever], weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "# adding the semantically split docs into the vector store\n",
    "vector_store_2_retriever.add_documents(semantic_docs)\n",
    "\n",
    "ensemble_retriever.invoke(\"How many credit hours is a major in Computer Science?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to have a base option that splits the text using just the RecursiveCharacterTextSplitter like the [original repo](https://github.com/yawbtng/SMUChatBot_Project/blob/main/main.py) does. From there, we're creating a third vector store collection to upload this text into using the vector store as the retriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, \n",
    "                                                length_function=len, add_start_index=True)  \n",
    "normal_split_docs = base_text_splitter.split_documents(docs)\n",
    "\n",
    "# checking result\n",
    "print(normal_split_docs[0].page_content)\n",
    "print(len(normal_split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting the collection name of the third vector store\n",
    "qdrant_collection_0 = os.environ['QDRANT_COLLECTION_0']\n",
    "\n",
    "# creating the third vector store and retriever\n",
    "vector_store_0 = create_vectorstore(qdrant_collection_0)\n",
    "vector_store_0_retriever = vector_store_0.as_retriever()\n",
    "\n",
    "# adding the recursively split docs into the vector store\n",
    "vector_store_0_retriever.add_documents(normal_split_docs)\n",
    "\n",
    "# testing the retriever\n",
    "vector_store_0_retriever.invoke(\"How many credit hours is a major in Computer Science?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Breakdown\n",
    "Now we are going to work on uploading the excel file of 115 FAQs into each vector database collection. There are multiple sheets within the excel file so we're going to have to turn each sheet into a CSV using the pandas library and then use langchain's CSV loader to turn them into langchain documents.\n",
    "\n",
    "The CSVs you see there now were created by iterating through the xlsx file so you can delete those if you want to see them being recreated (be careful not to delete the xlsx file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pandas library to work with the excel file and convert it to a data frame\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_path = 'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/SMU FAQs.xlsx'\n",
    "xlsx = pd.ExcelFile(excel_path)\n",
    "\n",
    "# checking to see if loading the file worked\n",
    "print(xlsx.sheet_names)\n",
    "\n",
    "# Iterate through each sheet and save as a CSV file\n",
    "csv_files = []\n",
    "for sheet_name in xlsx.sheet_names:\n",
    "    # Read the entire sheet to extract the metadata from cell A1\n",
    "    sheet_df = pd.read_excel(xlsx, sheet_name=sheet_name, header=None)\n",
    "    \n",
    "    # getting the link of the webpage to include as the metadata \n",
    "    metadata = sheet_df.iat[0, 0]\n",
    "    \n",
    "    # Read the sheet into a DataFrame starting from the second row\n",
    "    df = pd.read_excel(xlsx, sheet_name=sheet_name, skiprows=1)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = f'C:/Users/yawbt/OneDrive/Documents/GitHub/SURF-Project_Optimizing-PerunaBot/Data/{sheet_name}.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    csv_files.append((csv_path, metadata))\n",
    "\n",
    "# Display the list of generated CSV files and their metadata\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now turning each csv into a langchain document\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Create LangChain documents from CSV files with metadata\n",
    "csv_documents = []\n",
    "\n",
    "for csv_path, metadata in csv_files:\n",
    "    loader = CSVLoader(file_path=csv_path)\n",
    "    csv_docs = loader.load()\n",
    "    for csv_doc in csv_docs:\n",
    "        csv_doc.metadata['source'] = metadata\n",
    "    csv_documents.extend(csv_docs)\n",
    "\n",
    "# Display the first document as an example\n",
    "print(csv_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lastly upload the csv documents into each vector store collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store collection 1 - uses parent/child text splitter with parent retriever\n",
    "parent_retriever.add_documents(csv_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store collection 2 - uses semantic text splitter (or chunker) with the ensemble retriever (BM25 + vector store as retriever)\n",
    "# uploaded to vector store using vector store as the retriever\n",
    "vector_store_2_retriever.add_documents(csv_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector stoer collection 0 - uses the recursive chatacter text splitter with vector store as the retriever\n",
    "# base option from last project\n",
    "vector_store_0_retriever.add_documents(csv_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to create functions for variables we will need in other notebooks or python scripts or notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_langchain_docs():\n",
    "  return {\n",
    "    \"pdf_docs\": docs,\n",
    "    \"csv_docs\": csv_documents\n",
    "  }\n",
    "\n",
    "def get_all_vectorstores():\n",
    "  return {\n",
    "      \"vector_store_0\": vector_store_0, # collection smu-data_0\n",
    "      \"vector_store_1\": vector_store_1, # collection smu-data_1\n",
    "      \"vector_store_2\": vector_store_2, # collection smu-data_2\n",
    "  }\n",
    "\n",
    "def get_all_retrievers():\n",
    "  return {\n",
    "      \"vector_store_0_retriever\": vector_store_0_retriever, # collection smu-data_0\n",
    "      \"parent_retriever\": parent_retriever, # collection smu-data_1\n",
    "      \"ensemble_retriever\": ensemble_retriever, # collection smu-data_2\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are officially done with the data-preprocessing step and can move on to the RAG pipelines for each model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
