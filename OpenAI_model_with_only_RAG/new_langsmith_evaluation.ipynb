{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating each RAG pipeline using langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from the .env file using 'from dotenv import find_dotenv, load_dotenv'\n",
    "load_dotenv(find_dotenv(filename='SURF-Project_Optimizing-PerunaBot/setup/.env'))\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the 4 different RAG pipelines we made for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OG_PerunaBot_chain import Original_PerunaBot_eval_chain\n",
    "from chain_0 import base_retriever_eval_chain_0\n",
    "from chain_1 import parent_retriever_eval_chain_1\n",
    "from chain_2 import ensemble_retriever_eval_chain_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Target task definition\n",
    "\n",
    "# The name or UUID of the LangSmith dataset to evaluate on.\n",
    "\n",
    "data = \"SMU Schools QA\"\n",
    "\n",
    "# A string to prefix the experiment name with.\n",
    "# If not provided, a random string will be generated.\n",
    "experiment_prefix = \"SMU Schools QA\"\n",
    "\n",
    "# List of evaluators to score the outputs of target task\n",
    "evaluators = [\n",
    "  LangChainStringEvaluator(\"cot_qa\"),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"detail\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"coherence\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"relevance\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"helpfulness\"})\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_OG_chain(inputs: dict):\n",
    "    response = Original_PerunaBot_eval_chain.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_0(inputs: dict):\n",
    "    response = base_retriever_eval_chain_0.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_1(inputs: dict):\n",
    "    response = parent_retriever_eval_chain_1.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_2(inputs: dict):\n",
    "    response = ensemble_retriever_eval_chain_2.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'SMU Schools QA on OG PerunaBot chain-a304c553' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/7a23c577-228e-4b2b-a8af-c9e0a13c1625/compare?selectedSessions=ecf88071-640f-4256-974d-9fd4d1a97eac\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f4f8b7b2014c7cb7034e48eccd6b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 98231041-a269-4f41-a1d5-2968d0514a2e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9956, Requested 455. Please try again in 2.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9956, Requested 455. Please try again in 2.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e5a49a2f-db19-47f9-af4a-0935398a1364: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9960, Requested 417. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9960, Requested 417. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c1d04d3f-dedd-4ea9-b3d0-fd716613e0eb: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9832, Requested 411. Please try again in 1.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9832, Requested 411. Please try again in 1.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the target task\n",
    "OG_PerunaBot_langsmith_eval = evaluate(\n",
    "  predict_OG_chain,\n",
    "  data=data,\n",
    "  evaluators=evaluators,\n",
    "  experiment_prefix=experiment_prefix + \" on OG PerunaBot chain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'SMU Schools QA on chain 0-84b421e8' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/7a23c577-228e-4b2b-a8af-c9e0a13c1625/compare?selectedSessions=6e309932-01f3-4527-8494-e22d3aea26a3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d1749f3df94fa6917ba5c03a3ca5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run fa9943e0-e122-434b-8408-9b13e89eb842: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9987, Requested 447. Please try again in 2.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9987, Requested 447. Please try again in 2.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8ade7397-37cf-4532-b325-67da96b48b12: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9916, Requested 516. Please try again in 2.592s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9916, Requested 516. Please try again in 2.592s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run ac93a675-ddfe-4fe2-80ca-c197510136db: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9916, Requested 521. Please try again in 2.622s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9916, Requested 521. Please try again in 2.622s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run ac93a675-ddfe-4fe2-80ca-c197510136db: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9895, Requested 530. Please try again in 2.55s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9895, Requested 530. Please try again in 2.55s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73755ae2-aed2-4cb9-b52d-bc1faa021cf2: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9560, Requested 461. Please try again in 125ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9560, Requested 461. Please try again in 125ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 0802c07e-5f77-49dc-95a5-a768610b3644: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9940, Requested 485. Please try again in 2.55s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9940, Requested 485. Please try again in 2.55s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "chain_0_langsmith_eval = evaluate(\n",
    "    predict_chain_0,\n",
    "    data=data,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=experiment_prefix + \" on chain 0\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'SMU Schools QA on chain 1-218c12d4' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/7a23c577-228e-4b2b-a8af-c9e0a13c1625/compare?selectedSessions=f3f2b3db-eb84-40b3-89b5-15243b76b76c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f77b8828c34ee6a940e3cf17be1f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 85141248-c6ff-43c9-a196-1e40471c7212: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9974, Requested 471. Please try again in 2.67s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9974, Requested 471. Please try again in 2.67s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2b2ae51a-c06b-4f6f-b464-132a03233b1a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9590, Requested 498. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9590, Requested 498. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 85141248-c6ff-43c9-a196-1e40471c7212: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9933, Requested 480. Please try again in 2.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9933, Requested 480. Please try again in 2.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2b2ae51a-c06b-4f6f-b464-132a03233b1a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9631, Requested 507. Please try again in 828ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9631, Requested 507. Please try again in 828ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run ea62d84c-d8bb-462c-8eb8-6c0f3f6a7f43: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9830, Requested 591. Please try again in 2.526s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9830, Requested 591. Please try again in 2.526s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 5cf14db0-17d2-482c-b2bb-68362b97a206: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9981, Requested 459. Please try again in 2.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9981, Requested 459. Please try again in 2.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_1_langsmith_eval = evaluate(\n",
    "    predict_chain_1,\n",
    "    data=data,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=experiment_prefix + \" on chain 1\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'SMU Schools QA on chain 2-bc545425' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/7a23c577-228e-4b2b-a8af-c9e0a13c1625/compare?selectedSessions=741bccb0-8b19-42a9-a919-523e38be7eb7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44bc0375f754941947fc2747c43fbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 1efeb8bd-5d78-4f16-aaad-5a89628dbedd: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9973, Requested 458. Please try again in 2.586s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9973, Requested 458. Please try again in 2.586s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8d5c8e7d-6f4c-46ea-a7c4-7bc423213c41: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9978, Requested 471. Please try again in 2.694s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9978, Requested 471. Please try again in 2.694s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 95418e1e-637c-4470-9f09-eedced3612f9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9934, Requested 598. Please try again in 3.192s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9934, Requested 598. Please try again in 3.192s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8d5c8e7d-6f4c-46ea-a7c4-7bc423213c41: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9972, Requested 480. Please try again in 2.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9972, Requested 480. Please try again in 2.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 51dcdcaa-1456-460f-91f9-8d3de7ad86dc: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9649, Requested 479. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9649, Requested 479. Please try again in 768ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 21e02e27-1267-47ec-b863-cdac7927f78e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9977, Requested 461. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9977, Requested 461. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 95418e1e-637c-4470-9f09-eedced3612f9: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9902, Requested 607. Please try again in 3.054s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 220, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 550, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 589, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 646, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 10000, Used 9902, Requested 607. Please try again in 3.054s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_2_langsmith_eval = evaluate(\n",
    "    predict_chain_2,\n",
    "    data=data,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=experiment_prefix + \" on chain 2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain as RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_utilization\n",
    ")\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_util_chain = RagasEvaluatorChain(metric=context_utilization)\n",
    "\n",
    "ragas_evaluators = [\n",
    "    faithfulness_chain,\n",
    "    answer_rel_chain,\n",
    "    context_util_chain\n",
    "]\n",
    "\n",
    "dataset_2 = \"RAGAS Testset QA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_OG_chain_with_context(input: dict):\n",
    "    response = Original_PerunaBot_eval_chain.invoke({\"question\": input[\"question\"]})\n",
    "    return {\"answer\": response[\"output\"], \"contexts\": response[\"context\"]}\n",
    "\n",
    "def predict_chain_0_with_context(input: dict):\n",
    "    response = base_retriever_eval_chain_0.invoke({\"question\": input[\"question\"]})\n",
    "    return {\"answer\": response[\"output\"], \"contexts\": response[\"context\"]}\n",
    "\n",
    "def predict_chain_1_with_context(input: dict):\n",
    "    response = parent_retriever_eval_chain_1.invoke({\"question\": input[\"question\"]})\n",
    "    return {\"answer\": response[\"output\"], \"contexts\": response[\"context\"]}\n",
    "\n",
    "def predict_chain_2_with_context(input: dict):\n",
    "    response = ensemble_retriever_eval_chain_2.invoke({\"question\": input[\"question\"]})\n",
    "    return {\"answer\": response[\"output\"], \"contexts\": response[\"context\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the target task\n",
    "OG_PerunaBot_ragas_langsmith_eval = evaluate(\n",
    "  predict_OG_chain_with_context,\n",
    "  data=dataset_2,\n",
    "  evaluators=ragas_evaluators,\n",
    "  experiment_prefix=\"OG PerunaBot chain\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 0-8d0b3f5a' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/3643247a-7c6c-40d9-927e-ae50f5055df5/compare?selectedSessions=b7c5970b-a793-4d6a-968f-44537eba5bd2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6388e5eaf27643be9d2d125decee8137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 28136, Requested 2975. Please try again in 2.222s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 9b39066d-ec9e-4029-8fc0-d37b46db6ccf: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 9b39066d-ec9e-4029-8fc0-d37b46db6ccf: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 9b39066d-ec9e-4029-8fc0-d37b46db6ccf: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 25718ca4-3fe2-4309-a071-996a5ab553ca: AttributeError(\"partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 246, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1539, in _request\n",
      "    self._platform = await asyncify(get_platform)()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_sync.py\", line 75, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 25718ca4-3fe2-4309-a071-996a5ab553ca: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run cd89217a-94f0-49b2-a1a4-98b06f685880: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run cd89217a-94f0-49b2-a1a4-98b06f685880: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 8d0aa32f-73da-4118-af4b-900a6d46ae44: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 8d0aa32f-73da-4118-af4b-900a6d46ae44: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 68ea9cfc-0ba2-4bc2-8bbb-74efdcfeabd6: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 68ea9cfc-0ba2-4bc2-8bbb-74efdcfeabd6: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 1607337f-a0a7-489f-803f-4018da756427: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 1607337f-a0a7-489f-803f-4018da756427: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 31d61ac8-c66c-44f3-8266-8cc57d5f8731: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 31d61ac8-c66c-44f3-8266-8cc57d5f8731: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 10b4387b-660c-4cc9-9b3d-8aa7a789b7c0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 10b4387b-660c-4cc9-9b3d-8aa7a789b7c0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 27126f18-2482-4fd6-861f-5e6c14af48f7: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 27126f18-2482-4fd6-861f-5e6c14af48f7: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run a804c063-98ae-4317-8568-f299315672fa: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 0e76f3ef-2889-417a-9c7b-50a1a76217c9: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run a804c063-98ae-4317-8568-f299315672fa: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 0e76f3ef-2889-417a-9c7b-50a1a76217c9: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 6c6bfb20-4849-46cd-a929-c4d1d09306ab: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 6c6bfb20-4849-46cd-a929-c4d1d09306ab: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run eeefa66e-00fb-46fd-a5f5-832225bf30bc: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run eeefa66e-00fb-46fd-a5f5-832225bf30bc: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 27126f18-2482-4fd6-861f-5e6c14af48f7: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000022806EB27D0 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run a804c063-98ae-4317-8568-f299315672fa: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000022806C65B10 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 9a82463a-4e3c-4010-8ec1-ce1bc063b861: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 9a82463a-4e3c-4010-8ec1-ce1bc063b861: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run 16fdfc17-1359-4bfd-b358-4dda292008f0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 16fdfc17-1359-4bfd-b358-4dda292008f0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000022801C20DD0>, max_retries=1, _reproducibility=1)) on run ecdec3e2-4a66-4463-8d6f-35a9cc3d24e0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000002280682BD50>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run ecdec3e2-4a66-4463-8d6f-35a9cc3d24e0: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_0_ragas_langsmith_eval = evaluate(\n",
    "    predict_chain_0_with_context,\n",
    "    data=dataset_2,\n",
    "    evaluators=ragas_evaluators,\n",
    "    experiment_prefix=\"chain 0\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 1-248e040c' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/3643247a-7c6c-40d9-927e-ae50f5055df5/compare?selectedSessions=d43cb0fe-b6b9-4599-9f7b-d644e19674b1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8612a5d7247b45238c5f1cff58348521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 27254, Requested 3209. Please try again in 926ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 29767, Requested 3257. Please try again in 6.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 27944, Requested 3315. Please try again in 2.518s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 05a3a02d-c9b9-46d0-90c5-be69bfe5967e: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run ef80e375-7bb9-4379-882c-1cb0183b0be4: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 05a3a02d-c9b9-46d0-90c5-be69bfe5967e: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run ef80e375-7bb9-4379-882c-1cb0183b0be4: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 05a3a02d-c9b9-46d0-90c5-be69bfe5967e: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run ef80e375-7bb9-4379-882c-1cb0183b0be4: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run c945558e-5a5a-4430-87e1-72b176d4c964: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run c945558e-5a5a-4430-87e1-72b176d4c964: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run c945558e-5a5a-4430-87e1-72b176d4c964: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 907167fb-7c9d-4ecb-8d0c-2644948a3378: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 907167fb-7c9d-4ecb-8d0c-2644948a3378: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 030aef9a-0e09-4c0d-823e-35905b19615f: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 030aef9a-0e09-4c0d-823e-35905b19615f: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run f0c554e1-6ca8-49c2-b063-33426bb9aaff: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run f0c554e1-6ca8-49c2-b063-33426bb9aaff: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 26de321d-6ba2-43e1-bc33-a4bf2bbcbc8e: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 26de321d-6ba2-43e1-bc33-a4bf2bbcbc8e: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 6eee1f38-d864-4b1f-a923-00b39c5303e3: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 29897ffd-8ffe-4027-8f30-b69d13de1a6f: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 6eee1f38-d864-4b1f-a923-00b39c5303e3: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 29897ffd-8ffe-4027-8f30-b69d13de1a6f: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run e91d4c30-53fa-4896-839e-a95d55fddbb4: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run e91d4c30-53fa-4896-839e-a95d55fddbb4: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run b06870a1-f681-4dbd-a516-612da43fa473: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run b06870a1-f681-4dbd-a516-612da43fa473: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run cec146b9-57c3-486a-968f-7b10bfe81904: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run cec146b9-57c3-486a-968f-7b10bfe81904: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run b305b66b-1cc5-4d59-9911-31584c01d82c: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run b305b66b-1cc5-4d59-9911-31584c01d82c: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run ae380b5a-37f0-48f2-9b99-dc22811ae4ad: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run ae380b5a-37f0-48f2-9b99-dc22811ae4ad: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run 252aef18-44bd-485d-bb86-69175b86737e: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 252aef18-44bd-485d-bb86-69175b86737e: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run cec146b9-57c3-486a-968f-7b10bfe81904: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x00000203783D3110 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run b06870a1-f681-4dbd-a516-612da43fa473: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x00000203786A5390 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x0000020376F94B10>, max_retries=1, _reproducibility=1)) on run ecf37d1b-0289-4368-b088-daf15a37bddd: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x0000020377F22F10>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run ecf37d1b-0289-4368-b088-daf15a37bddd: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_1_ragas_langsmith_eval = evaluate(\n",
    "    predict_chain_1_with_context,\n",
    "    data=dataset_2,\n",
    "    evaluators=ragas_evaluators,\n",
    "    experiment_prefix=\"chain 1\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 2-c52d3c61' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/3643247a-7c6c-40d9-927e-ae50f5055df5/compare?selectedSessions=fbfa5c22-9704-4cbe-b672-dc68e93f4551\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5265832b12534b99b05ae8fa843c1f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 29163, Requested 3750. Please try again in 5.826s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 29051, Requested 3899. Please try again in 5.9s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 27343, Requested 2703. Please try again in 92ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Hkbsf8yAWp77cZYrVZcR2Dim on tokens per min (TPM): Limit 30000, Used 27568, Requested 4922. Please try again in 4.98s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run b703bf1e-e3a8-4b6c-91fc-9ef121d3d433: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run d8c213dd-757b-4861-9140-5436c989da52: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run b703bf1e-e3a8-4b6c-91fc-9ef121d3d433: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run d8c213dd-757b-4861-9140-5436c989da52: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run b703bf1e-e3a8-4b6c-91fc-9ef121d3d433: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run d8c213dd-757b-4861-9140-5436c989da52: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 4be74be8-654f-467c-af4d-514aec9b533a: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 4be74be8-654f-467c-af4d-514aec9b533a: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 4be74be8-654f-467c-af4d-514aec9b533a: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 74d81e2d-76be-4520-b5f2-57e9ed1fdafd: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 74d81e2d-76be-4520-b5f2-57e9ed1fdafd: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 74d81e2d-76be-4520-b5f2-57e9ed1fdafd: ValueError(\"Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 190, in evaluate_run\n",
      "    self._validate_langsmith_eval(run, example)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 179, in _validate_langsmith_eval\n",
      "    raise ValueError(\n",
      "ValueError: Expected 'answer' and 'contexts' in run.outputs.Got: ['output']\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run c20095f3-3050-4457-9b82-9234cf25f612: AttributeError(\"partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 246, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1539, in _request\n",
      "    self._platform = await asyncify(get_platform)()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_sync.py\", line 75, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run c23255a5-b2db-4676-bdc4-20eac70dec9d: AttributeError(\"partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 246, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1539, in _request\n",
      "    self._platform = await asyncify(get_platform)()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_utils\\_sync.py\", line 75, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'anyio._backends._asyncio' has no attribute 'run_sync_in_worker_thread' (most likely due to a circular import)\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run c20095f3-3050-4457-9b82-9234cf25f612: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run c23255a5-b2db-4676-bdc4-20eac70dec9d: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run e49d8f9a-1e3d-4a41-a966-aae447d3e9e7: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 2712e62a-a16e-4594-9ea0-62807d6c086c: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run e49d8f9a-1e3d-4a41-a966-aae447d3e9e7: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 2712e62a-a16e-4594-9ea0-62807d6c086c: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 01098174-a1e8-418e-9cca-2a7199f00263: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 01098174-a1e8-418e-9cca-2a7199f00263: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 4303b22e-f421-4f49-a14a-7d5428345541: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 4303b22e-f421-4f49-a14a-7d5428345541: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 24659dd3-e92b-4bc7-bec1-2a4a59640ff1: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 24659dd3-e92b-4bc7-bec1-2a4a59640ff1: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 942b4a68-9320-463f-8f3f-ee2f06fba4fb: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 942b4a68-9320-463f-8f3f-ee2f06fba4fb: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 680fdd69-3032-4cef-b8fc-532df7ddc7d9: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 680fdd69-3032-4cef-b8fc-532df7ddc7d9: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 629f348b-ef6e-425d-bda6-9cee047fb878: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 629f348b-ef6e-425d-bda6-9cee047fb878: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 6eef6ccd-84ff-4491-876c-066c94854b6a: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 6eef6ccd-84ff-4491-876c-066c94854b6a: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=Faithfulness(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='faithfulness', evaluation_mode=<EvaluationMode.qac: 1>, nli_statements_message=Prompt(name='nli_statements', instruction='Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}}, \"required\": [\"statement\", \"reason\", \"verdict\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', 'statements': ['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.'], 'answer': [{'statement': 'John is majoring in Biology.', 'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", 'verdict': 0}, {'statement': 'John is taking a course on Artificial Intelligence.', 'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', 'verdict': 0}, {'statement': 'John is a dedicated student.', 'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', 'verdict': 1}, {'statement': 'John has a part-time job.', 'reason': 'There is no information given in the context about John having a part-time job.', 'verdict': 0}]}, {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', 'statements': ['Albert Einstein was a genius.'], 'answer': [{'statement': 'Albert Einstein was a genius.', 'reason': 'The context and statement are unrelated', 'verdict': 0}]}], input_keys=['context', 'statements'], output_key='answer', output_type='json', language='english'), statement_prompt=Prompt(name='long_form_answer', instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\", output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Statements\"}, \"definitions\": {\"Statements\": {\"title\": \"Statements\", \"type\": \"object\", \"properties\": {\"sentence_index\": {\"title\": \"Sentence Index\", \"description\": \"Index of the sentence from the statement list\", \"type\": \"integer\"}, \"simpler_statements\": {\"title\": \"Simpler Statements\", \"description\": \"the simpler statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"sentence_index\", \"simpler_statements\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'Who was Albert Einstein and what is he best known for?', 'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', 'sentences': '\\n        0:He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. \\n        1:He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\\n        ', 'analysis': [{'sentence_index': 0, 'simpler_statements': ['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']}, {'sentence_index': 1, 'simpler_statements': ['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']}]}], input_keys=['question', 'answer', 'sentences'], output_key='analysis', output_type='json', language='english'), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001A09181CA50>, max_retries=1, _reproducibility=1)) on run 79b29f65-48ea-47c2-8440-c26bfd278428: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 262, in _ascore\n",
      "    p_value = self._create_nli_prompt(row, statements)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 204, in _create_nli_prompt\n",
      "    contexts_str: str = \"\\n\".join(contexts)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x000001A08FAA0850>, llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)) on run 79b29f65-48ea-47c2-8440-c26bfd278428: TypeError('sequence item 0: expected str instance, Document found')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 151, in _ascore\n",
      "    prompt = self._create_question_gen_prompt(row)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 146, in _create_question_gen_prompt\n",
      "    return self.question_generation.format(answer=ans, context=\"\\n\".join(ctx))\n",
      "                                                               ^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, Document found\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run e49d8f9a-1e3d-4a41-a966-aae447d3e9e7: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x000001A0901ABFD0 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Error running evaluator EvaluatorChain(metric=ContextUtilization(llm=LangchainLLMWrapper(run_config=RunConfig(timeout=60, max_retries=10, max_wait=60, max_workers=16, thread_timeout=80.0, exception_types=<class 'openai.RateLimitError'>, log_tenacity=False)), name='context_utilization', evaluation_mode=<EvaluationMode.qac: 1>, context_precision_prompt=Prompt(name='context_precision', instruction='Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output.', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"Answer for the verification task wether the context was useful.\", \"type\": \"object\", \"properties\": {\"reason\": {\"title\": \"Reason\", \"description\": \"Reason for verification\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"Binary (0/1) verdict of verification\", \"type\": \"integer\"}}, \"required\": [\"reason\", \"verdict\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'question': 'What can you tell me about albert Albert Einstein?', 'context': 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.', 'answer': 'Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895', 'verification': {'reason': \"The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.\", 'verdict': 1}}, {'question': 'who won 2020 icc world cup?', 'context': \"The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.\", 'answer': 'England', 'verification': {'reason': 'the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.', 'verdict': 1}}, {'question': 'What is the tallest mountain in the world?', 'context': 'The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.', 'answer': 'Mount Everest.', 'verification': {'reason': \"the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.\", 'verdict': 0}}], input_keys=['question', 'context', 'answer'], output_key='verification', output_type='json', language='english'), max_retries=1, _reproducibility=1)) on run 629f348b-ef6e-425d-bda6-9cee047fb878: APIConnectionError('Connection error.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 196, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 138, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1203, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x000001A090130290 [unset]> is bound to a different event loop\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 105, in score\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    score = asyncio.run(self._ascore(row=row, callbacks=group_cm))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 160, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 673, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\yawbt\\anaconda3\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 858, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 740, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1582, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1651, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1592, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_2_ragas_langsmith_eval = evaluate(\n",
    "    predict_chain_2_with_context,\n",
    "    data=dataset_2,\n",
    "    evaluators=ragas_evaluators,\n",
    "    experiment_prefix=\"chain 2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Target task definition\n",
    "\n",
    "# The name or UUID of the LangSmith dataset to evaluate on.\n",
    "# Alternatively, you can pass an iterator of examples\n",
    "dataset_3 = \"SMU Schools Questions\"\n",
    "\n",
    "\n",
    "# List of evaluators to score the outputs of target task\n",
    "langsmith_evaluators = [\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"detail\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"coherence\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"relevance\"}),\n",
    "  LangChainStringEvaluator(\"labeled_criteria\", config={\"criteria\": \"helpfulness\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_OG_chain(inputs: dict):\n",
    "    response = Original_PerunaBot_eval_chain.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_0(inputs: dict):\n",
    "    response = base_retriever_eval_chain_0.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_1(inputs: dict):\n",
    "    response = parent_retriever_eval_chain_1.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}\n",
    "\n",
    "def predict_chain_2(inputs: dict):\n",
    "    response = ensemble_retriever_eval_chain_2.invoke({\"question\": inputs[\"question\"]})\n",
    "    return {\"answer\" : response[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'OG PerunaBot chain-39986212' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=77674d36-5c7f-40ab-aac5-9ade509cd489\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7eda781be643bab06a3a874cb839de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e5b01709-c139-4df4-be2f-b067cbf3c941: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2363ec5-c4ee-46a3-95f9-804ea4405acb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 935c3984-4e80-4d97-a3e2-1dbb42a4ce79: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f44fba6b-a717-4848-bff3-a78d59b728f2: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8fe11ebe-5391-4d21-aea8-b5e0dfd43acd: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 75844b64-0804-452b-a5a2-f0b78900937d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73d428f7-9312-4811-8ace-fab6ec462783: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 666b3bc5-b617-4154-acf3-b78cf4b65be7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e5b01709-c139-4df4-be2f-b067cbf3c941: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2363ec5-c4ee-46a3-95f9-804ea4405acb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 935c3984-4e80-4d97-a3e2-1dbb42a4ce79: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f44fba6b-a717-4848-bff3-a78d59b728f2: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8fe11ebe-5391-4d21-aea8-b5e0dfd43acd: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 75844b64-0804-452b-a5a2-f0b78900937d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73d428f7-9312-4811-8ace-fab6ec462783: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 666b3bc5-b617-4154-acf3-b78cf4b65be7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e5b01709-c139-4df4-be2f-b067cbf3c941: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2363ec5-c4ee-46a3-95f9-804ea4405acb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 935c3984-4e80-4d97-a3e2-1dbb42a4ce79: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f44fba6b-a717-4848-bff3-a78d59b728f2: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8fe11ebe-5391-4d21-aea8-b5e0dfd43acd: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 75844b64-0804-452b-a5a2-f0b78900937d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73d428f7-9312-4811-8ace-fab6ec462783: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 666b3bc5-b617-4154-acf3-b78cf4b65be7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e5b01709-c139-4df4-be2f-b067cbf3c941: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2363ec5-c4ee-46a3-95f9-804ea4405acb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 935c3984-4e80-4d97-a3e2-1dbb42a4ce79: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f44fba6b-a717-4848-bff3-a78d59b728f2: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 8fe11ebe-5391-4d21-aea8-b5e0dfd43acd: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 75844b64-0804-452b-a5a2-f0b78900937d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73d428f7-9312-4811-8ace-fab6ec462783: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 666b3bc5-b617-4154-acf3-b78cf4b65be7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n"
     ]
    }
   ],
   "source": [
    "OG_PerunaBot_langsmith_eval_2 = evaluate(\n",
    "    predict_OG_chain,\n",
    "    data=dataset_3,\n",
    "    evaluators=langsmith_evaluators,\n",
    "    experiment_prefix=\"OG PerunaBot chain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 0-1f26194d' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=40dba849-ef43-4fc2-85f6-2ac2fb331aeb\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b430fae7dc274c48aefb49421771e23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 67c3d8ce-b579-4d1e-8e53-ccb99043cb75: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7e27fc-8d2c-4e2b-8be8-96cb486be42d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c778d699-8295-4d2c-9c4b-467adfb5b064: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d2e3c2e3-a083-4f9e-9e15-2d9e1e93c462: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run a544d6c2-7c9a-4fd9-888b-d348f4760abb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f88733fc-6c56-40f1-8f74-32f42e7f8fd7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 09a85b47-11c8-48e7-81f3-4a2ddd7fb3e8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 67c3d8ce-b579-4d1e-8e53-ccb99043cb75: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d32dc7a5-64db-4a65-9392-2c4d8a8b7de3: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7e27fc-8d2c-4e2b-8be8-96cb486be42d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c778d699-8295-4d2c-9c4b-467adfb5b064: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d2e3c2e3-a083-4f9e-9e15-2d9e1e93c462: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run a544d6c2-7c9a-4fd9-888b-d348f4760abb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f88733fc-6c56-40f1-8f74-32f42e7f8fd7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 09a85b47-11c8-48e7-81f3-4a2ddd7fb3e8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 67c3d8ce-b579-4d1e-8e53-ccb99043cb75: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d32dc7a5-64db-4a65-9392-2c4d8a8b7de3: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7e27fc-8d2c-4e2b-8be8-96cb486be42d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c778d699-8295-4d2c-9c4b-467adfb5b064: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d2e3c2e3-a083-4f9e-9e15-2d9e1e93c462: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run a544d6c2-7c9a-4fd9-888b-d348f4760abb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f88733fc-6c56-40f1-8f74-32f42e7f8fd7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 09a85b47-11c8-48e7-81f3-4a2ddd7fb3e8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 67c3d8ce-b579-4d1e-8e53-ccb99043cb75: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d32dc7a5-64db-4a65-9392-2c4d8a8b7de3: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4d7e27fc-8d2c-4e2b-8be8-96cb486be42d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run c778d699-8295-4d2c-9c4b-467adfb5b064: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d2e3c2e3-a083-4f9e-9e15-2d9e1e93c462: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run a544d6c2-7c9a-4fd9-888b-d348f4760abb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f88733fc-6c56-40f1-8f74-32f42e7f8fd7: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 09a85b47-11c8-48e7-81f3-4a2ddd7fb3e8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d32dc7a5-64db-4a65-9392-2c4d8a8b7de3: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_0_langsmith_eval_2 = evaluate(\n",
    "    predict_chain_0,\n",
    "    data=dataset_3,\n",
    "    evaluators=langsmith_evaluators,\n",
    "    experiment_prefix=\"chain 0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 1-30610d12' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=0f8bfae4-12be-4d93-becd-9d480a978689\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad0693178164baeb8e3a90c2d8705e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86e982a7-a2c4-40cb-b19d-6ee6f2aff587: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run fffe0baf-724b-4239-ab1e-4d9c4ae9393b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86b68797-93c8-4d94-9c94-dc1d0a60dcc0: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 5ce1b5b0-0c32-4ad7-9ba7-cc94dabf2740: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run de14b9d0-da99-47ca-b80c-b89ae25c8b1f: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e06a6d22-940f-4ca5-97a0-809d0ed2aa7e: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2462bf5-9368-4a6d-bb75-a206cc8dfc81: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d27f6923-eb47-4246-ada2-d5d55e8caabb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86e982a7-a2c4-40cb-b19d-6ee6f2aff587: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run fffe0baf-724b-4239-ab1e-4d9c4ae9393b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86b68797-93c8-4d94-9c94-dc1d0a60dcc0: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 5ce1b5b0-0c32-4ad7-9ba7-cc94dabf2740: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run de14b9d0-da99-47ca-b80c-b89ae25c8b1f: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e06a6d22-940f-4ca5-97a0-809d0ed2aa7e: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2462bf5-9368-4a6d-bb75-a206cc8dfc81: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d27f6923-eb47-4246-ada2-d5d55e8caabb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86e982a7-a2c4-40cb-b19d-6ee6f2aff587: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run fffe0baf-724b-4239-ab1e-4d9c4ae9393b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86b68797-93c8-4d94-9c94-dc1d0a60dcc0: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 5ce1b5b0-0c32-4ad7-9ba7-cc94dabf2740: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run de14b9d0-da99-47ca-b80c-b89ae25c8b1f: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e06a6d22-940f-4ca5-97a0-809d0ed2aa7e: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2462bf5-9368-4a6d-bb75-a206cc8dfc81: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d27f6923-eb47-4246-ada2-d5d55e8caabb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86e982a7-a2c4-40cb-b19d-6ee6f2aff587: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run fffe0baf-724b-4239-ab1e-4d9c4ae9393b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 86b68797-93c8-4d94-9c94-dc1d0a60dcc0: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 5ce1b5b0-0c32-4ad7-9ba7-cc94dabf2740: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run de14b9d0-da99-47ca-b80c-b89ae25c8b1f: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run e06a6d22-940f-4ca5-97a0-809d0ed2aa7e: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run f2462bf5-9368-4a6d-bb75-a206cc8dfc81: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run d27f6923-eb47-4246-ada2-d5d55e8caabb: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_1_langsmith_eval_2 = evaluate(\n",
    "    predict_chain_1,\n",
    "    data=dataset_3,\n",
    "    evaluators=langsmith_evaluators,\n",
    "    experiment_prefix=\"chain 1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'chain 2-83a99ce8' at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=0d993ac0-9056-4302-828d-f57baa5f933d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957796059cc444da8f51cda53c0e494c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73769364-8264-4e01-9c6e-391062925a4b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 41b62245-1701-4632-82d5-cd1cb9cf3b7d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run daed3f65-c695-41a5-af52-2f95423fd53c: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run dd41fb95-f9e5-4974-9b6f-7f83f7d982d6: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run b5fb4ac8-c047-45d1-846a-c89356e2e5e1: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73769364-8264-4e01-9c6e-391062925a4b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 13a1eae0-a918-4f43-9129-c75a572367a8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4c1abba7-6a41-429c-8436-a11985478599: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 41b62245-1701-4632-82d5-cd1cb9cf3b7d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a120e01-762c-463f-9161-77add2f3b872: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run daed3f65-c695-41a5-af52-2f95423fd53c: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run dd41fb95-f9e5-4974-9b6f-7f83f7d982d6: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run b5fb4ac8-c047-45d1-846a-c89356e2e5e1: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73769364-8264-4e01-9c6e-391062925a4b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 13a1eae0-a918-4f43-9129-c75a572367a8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4c1abba7-6a41-429c-8436-a11985478599: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 41b62245-1701-4632-82d5-cd1cb9cf3b7d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a120e01-762c-463f-9161-77add2f3b872: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run daed3f65-c695-41a5-af52-2f95423fd53c: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run dd41fb95-f9e5-4974-9b6f-7f83f7d982d6: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run b5fb4ac8-c047-45d1-846a-c89356e2e5e1: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 73769364-8264-4e01-9c6e-391062925a4b: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 13a1eae0-a918-4f43-9129-c75a572367a8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4c1abba7-6a41-429c-8436-a11985478599: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 41b62245-1701-4632-82d5-cd1cb9cf3b7d: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a120e01-762c-463f-9161-77add2f3b872: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run daed3f65-c695-41a5-af52-2f95423fd53c: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run dd41fb95-f9e5-4974-9b6f-7f83f7d982d6: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run b5fb4ac8-c047-45d1-846a-c89356e2e5e1: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 13a1eae0-a918-4f43-9129-c75a572367a8: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 4c1abba7-6a41-429c-8436-a11985478599: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n",
      "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a120e01-762c-463f-9161-77add2f3b872: ValueError('LabeledCriteriaEvalChain requires a reference string.')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1258, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 278, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 582, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 579, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\integrations\\_langchain.py\", line 260, in evaluate\n",
      "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 219, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"c:\\Users\\yawbt\\OneDrive\\Documents\\GitHub\\SURF-Project_Optimizing-PerunaBot\\.venv\\Lib\\site-packages\\langchain\\evaluation\\schema.py\", line 127, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: LabeledCriteriaEvalChain requires a reference string.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain_2_langsmith_eval_2 = evaluate(\n",
    "    predict_chain_2,\n",
    "    data=dataset_3,\n",
    "    evaluators=langsmith_evaluators,\n",
    "    experiment_prefix=\"chain 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "prompts = {\n",
    "    \"prompt\" : hub.pull(\"langchain-ai/pairwise-evaluation-2\"),\n",
    "    \"rag prompt\" : hub.pull(\"langchain-ai/pairwise-evaluation-rag\"),\n",
    "    \"academic advisor prompt\" : hub.pull(\"perunabot-pairwise-evaluation\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Normal Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_pairwise_with_prompt(runs: list[Run], example: Example):\n",
    "    scores = {}\n",
    "    \n",
    "    # Create the model to run your evaluator\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "    \n",
    "    runnable = prompts[\"prompt\"] | model\n",
    "    response = runnable.invoke({\n",
    "        \"question\": example.inputs[\"question\"],\n",
    "        \"answer_a\": runs[0].outputs[\"answer\"] if runs[0].outputs is not None else \"N/A\",\n",
    "        \"answer_b\": runs[1].outputs[\"answer\"] if runs[1].outputs is not None else \"N/A\",\n",
    "    })\n",
    "    score = response[\"Preference\"]\n",
    "    if score == 1:\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == 2:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG PerunaBot vs Chain 0\n",
    "pairwise_eval_1 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 0-1f26194d\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 0\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 0\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 1\n",
    "pairwise_eval_2 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 1\",\n",
    "             \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 2\n",
    "pairwise_eval_3 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 1\n",
    "pairwise_eval_4 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 1\",\n",
    "             \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 2\n",
    "pairwise_eval_5 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")\n",
    "\n",
    "# Chain 1 vs Chain 2\n",
    "pairwise_eval_6 = evaluate_comparative(\n",
    "    [\"chain 1-30610d12\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_prompt],\n",
    "    experiment_prefix=\"Chain 1 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 1 vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-2\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "![Pairwise Evaluation Results](../Data/Evaluation%20Results/pairwise%20evaluation%20v1%207-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_pairwise_with_rag_prompt(runs: list[Run], example: Example):\n",
    "    scores = {}\n",
    "    \n",
    "    # Create the model to run your evaluator\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "    \n",
    "    runnable = prompts[\"rag prompt\"] | model\n",
    "    response = runnable.invoke({\n",
    "        \"question\": example.inputs[\"question\"],\n",
    "        \"answer_a\": runs[0].outputs[\"answer\"] if runs[0].outputs is not None else \"N/A\",\n",
    "        \"answer_b\": runs[1].outputs[\"answer\"] if runs[1].outputs is not None else \"N/A\",\n",
    "    })\n",
    "    score = response[\"Preference\"]\n",
    "    if score == 1:\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == 2:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG PerunaBot vs Chain 0\n",
    "pairwise_eval_1 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 0-1f26194d\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 0\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 0\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 1\n",
    "pairwise_eval_2 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 1\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 2\n",
    "pairwise_eval_3 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 1\n",
    "pairwise_eval_4 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 1\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 2\n",
    "pairwise_eval_5 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")\n",
    "\n",
    "# Chain 1 vs Chain 2\n",
    "pairwise_eval_6 = evaluate_comparative(\n",
    "    [\"chain 1-30610d12\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_rag_prompt],\n",
    "    experiment_prefix=\"Chain 1 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 1 vs Chain 2\",\n",
    "              \"prompt\" : \"langchain-ai/pairwise-evaluation-rag\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "![Pairwise Evaluation Results](../Data/Evaluation%20Results/pairwise%20evaluation%20v2%207-30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Academic Advisor Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_pairwise_with_advisor_prompt(runs: list[Run], example: Example):\n",
    "    scores = {}\n",
    "    \n",
    "    # Create the model to run your evaluator\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "    \n",
    "    runnable = prompts[\"academic advisor prompt\"] | model\n",
    "    response = runnable.invoke({\n",
    "        \"question\": example.inputs[\"question\"],\n",
    "        \"answer_a\": runs[0].outputs[\"answer\"] if runs[0].outputs is not None else \"N/A\",\n",
    "        \"answer_b\": runs[1].outputs[\"answer\"] if runs[1].outputs is not None else \"N/A\",\n",
    "    })\n",
    "    score = response[\"Preference\"]\n",
    "    if score == 1:\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == 2:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=77674d36-5c7f-40ab-aac5-9ade509cd489%2C40dba849-ef43-4fc2-85f6-2ac2fb331aeb&comparativeExperiment=1048b5ad-3b25-46ab-8e2b-27f4e5dc0a25\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b0fac66344fa780e9d610689d1f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=77674d36-5c7f-40ab-aac5-9ade509cd489%2C0f8bfae4-12be-4d93-becd-9d480a978689&comparativeExperiment=80d0a129-9728-4ae4-bbc7-976fdc81baf2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6337af5a5456485c917b74ec4f45d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=77674d36-5c7f-40ab-aac5-9ade509cd489%2C0d993ac0-9056-4302-828d-f57baa5f933d&comparativeExperiment=7f7c7de0-45c6-4970-aa46-a627fe8bc1bc\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68a7ce3b2cc4039a7209784d437f17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=40dba849-ef43-4fc2-85f6-2ac2fb331aeb%2C0f8bfae4-12be-4d93-becd-9d480a978689&comparativeExperiment=17f0e8f5-371c-407b-9af7-970943f6cb66\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c7dc147f7a469783408ab38451363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=40dba849-ef43-4fc2-85f6-2ac2fb331aeb%2C0d993ac0-9056-4302-828d-f57baa5f933d&comparativeExperiment=a985e125-8cf1-4990-9cdd-1422a91a0a3f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cce57473ed94417a56166018aeda205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/d6d3de7f-d3b8-5077-9835-ce02339ff6b9/datasets/5d401ffc-aab6-461d-9092-0648c24d8a80/compare?selectedSessions=0f8bfae4-12be-4d93-becd-9d480a978689%2C0d993ac0-9056-4302-828d-f57baa5f933d&comparativeExperiment=b3e2db54-c774-434c-b812-28ca43c74cd4\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693e0449d74840a499332c3bd83377f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OG PerunaBot vs Chain 0\n",
    "pairwise_eval_1 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 0-1f26194d\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 0\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 0\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 1\n",
    "pairwise_eval_2 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 1\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")\n",
    "\n",
    "# OG PerunaBot vs Chain 2\n",
    "pairwise_eval_3 = evaluate_comparative(\n",
    "    [\"OG PerunaBot chain-39986212\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"OG PerunaBot vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"OG PerunaBot vs Chain 2\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 1\n",
    "pairwise_eval_4 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 1-30610d12\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 1\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 1\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")\n",
    "\n",
    "# Chain 0 vs Chain 2\n",
    "pairwise_eval_5 = evaluate_comparative(\n",
    "    [\"chain 0-1f26194d\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"Chain 0 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 0 vs Chain 2\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")\n",
    "\n",
    "# Chain 1 vs Chain 2\n",
    "pairwise_eval_6 = evaluate_comparative(\n",
    "    [\"chain 1-30610d12\", \"chain 2-83a99ce8\"],\n",
    "    evaluators=[evaluate_pairwise_with_advisor_prompt],\n",
    "    experiment_prefix=\"Chain 1 vs Chain 2\",\n",
    "    randomize_order=True,\n",
    "    metadata={\"run name\": \"Chain 1 vs Chain 2\",\n",
    "              \"prompt\" : \"perunabot-pairwise-evaluation\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "![Pairwise Evaluation Results](../Data/Evaluation%20Results/pairwise%20evaluation%20v3%207-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data with corrected ties\n",
    "individual_wins = {\n",
    "    \"OG PerunaBot\": 7,\n",
    "    \"Chain 0\": 42,\n",
    "    \"Chain 1\": 38,\n",
    "    \"Chain 2\": 47\n",
    "}\n",
    "\n",
    "# Extracting data\n",
    "chains = list(individual_wins.keys())\n",
    "wins = list(individual_wins.values())\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(chains, wins, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Chains')\n",
    "plt.ylabel('Number of Wins')\n",
    "\n",
    "# Add text annotations to the plot\n",
    "for i in range(len(chains)):\n",
    "    plt.text(i, wins[i], str(wins[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Individual Wins of Each Chain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage of each chain (out of 72 total comparisons):\n",
      "OG PerunaBot: 9.72%\n",
      "Chain 0: 58.33%\n",
      "Chain 1: 52.78%\n",
      "Chain 2: 65.28%\n"
     ]
    }
   ],
   "source": [
    "# Data with corrected ties\n",
    "individual_wins = {\n",
    "    \"OG PerunaBot\": 7,\n",
    "    \"Chain 0\": 42,\n",
    "    \"Chain 1\": 38,\n",
    "    \"Chain 2\": 47\n",
    "}\n",
    "\n",
    "print(\"Win percentage of each chain (out of 72 total comparisons):\")\n",
    "for i in individual_wins:\n",
    "    win_percentage = ((individual_wins[i] / 72) * 100).__round__(2)\n",
    "    print(f\"{i}: {win_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data with wins based on thumbs up\n",
    "overall_wins = {\n",
    "    \"OG PerunaBot\": 0,\n",
    "    \"Chain 0\": 7,\n",
    "    \"Chain 1\": 4,\n",
    "    \"Chain 2\": 6\n",
    "}\n",
    "\n",
    "# Extracting data\n",
    "chains = list(overall_wins.keys())\n",
    "thumbs_up_wins = list(overall_wins.values())\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(chains, thumbs_up_wins, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Chains')\n",
    "plt.ylabel('Number of Wins')\n",
    "\n",
    "# Add text annotations to the plot\n",
    "for i in range(len(chains)):\n",
    "    plt.text(i, thumbs_up_wins[i], str(thumbs_up_wins[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Performance Comparison of RAG Pipelines (Thumbs Up Count)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
